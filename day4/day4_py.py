# -*- coding: utf-8 -*-
"""day4.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pu9GRwhdH0t09wezyAAPPoDLcMy5u3cy
"""

!pip install transformers sentence-transformers

from transformers import BertTokenizer, BertModel
from sentence_transformers import SentenceTransformer, util
import torch

model_name = 'bert-base-uncased'  # You can choose other BERT models
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)

def get_bert_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
    embeddings = outputs.last_hidden_state.mean(dim=1)
    return embeddings


def calculate_similarity(text1, text2):
    embedding1 = get_bert_embedding(text1)
    embedding2 = get_bert_embedding(text2)

    similarity_score = util.cos_sim(embedding1, embedding2).item()
    return similarity_score

text1 = "This is a positive sentence."
text2 = "This is another positive sentence."
text3 = "This is a negative sentence."

similarity_1_2 = calculate_similarity(text1, text2)
similarity_1_3 = calculate_similarity(text1, text3)


print(f"Similarity between '{text1}' and '{text2}': {similarity_1_2}")
print(f"Similarity between '{text1}' and '{text3}': {similarity_1_3}")

!pip install transformers sentence-transformers

from transformers import PreTrainedTokenizerFast, BertModel
from sentence_transformers import SentenceTransformer, util
import torch

# Load pre-trained ModernBERT model and tokenizer
model_name = 'answerdotai/ModernBERT-base'  # Changed to ModernBERT
tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)

# Or use a sentence transformer model directly for better performance
# model = SentenceTransformer('answerdotai/ModernBERT-base') # Example: ModernBERT-base


def get_bert_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
    embeddings = outputs.last_hidden_state.mean(dim=1)  # Average pooling of token embeddings
    return embeddings


def calculate_similarity(text1, text2):
    embedding1 = get_bert_embedding(text1)
    embedding2 = get_bert_embedding(text2)

    # Calculate cosine similarity
    similarity_score = util.cos_sim(embedding1, embedding2).item()
    return similarity_score


# Example usage
text1 = "This is a positive sentence."
text2 = "This is another positive sentence."
text3 = "This is a negative sentence."

similarity_1_2 = calculate_similarity(text1, text2)
similarity_1_3 = calculate_similarity(text1, text3)

print(f"Similarity between '{text1}' and '{text2}': {similarity_1_2}")
print(f"Similarity between '{text1}' and '{text3}': {similarity_1_3}")

!pip install gensim

from gensim.models import Word2Vec
from gensim.models import KeyedVectors
import numpy as np

# Sample sentences for training the Word2Vec model
sentences = [
    ["This", "is", "a", "positive", "sentence"],
    ["This", "is", "another", "positive", "sentence"],
    ["This", "is", "a", "negative", "sentence"],
    ["I", "love", "sunny", "days"],
    ["Rainy", "days", "make", "me", "feel", "sad"]
]

# Train a Word2Vec model
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)

# Save the model (optional)
model.save("word2vec.model")

def get_word2vec_embedding(text):
    words = text.split()
    word_vectors = [model.wv[word] for word in words if word in model.wv]
    if len(word_vectors) == 0:
        return np.zeros(model.vector_size)
    return np.mean(word_vectors, axis=0)

def calculate_similarity(text1, text2):
    embedding1 = get_word2vec_embedding(text1)
    embedding2 = get_word2vec_embedding(text2)

    # Calculate cosine similarity
    similarity_score = np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))
    return similarity_score

# Example usage with more sentences
text1 = "This is a positive sentence."
text2 = "This is another positive sentence."
text3 = "This is a negative sentence."
text4 = "I love sunny days."
text5 = "Rainy days make me feel sad."

similarity_1_2 = calculate_similarity(text1, text2)
similarity_1_3 = calculate_similarity(text1, text3)
similarity_1_4 = calculate_similarity(text1, text4)
similarity_4_5 = calculate_similarity(text4, text5)

print(f"Similarity between '{text1}' and '{text2}': {similarity_1_2}")
print(f"Similarity between '{text1}' and '{text3}': {similarity_1_3}")
print(f"Similarity between '{text1}' and '{text4}': {similarity_1_4}")
print(f"Similarity between '{text4}' and '{text5}': {similarity_4_5}")